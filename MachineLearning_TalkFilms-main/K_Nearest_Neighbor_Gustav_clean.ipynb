{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91953150-13e2-4444-b0b2-246794fdf5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import os\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.base      import clone\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from sklearn.metrics import * \n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a823110-215b-4b8f-a347-424abafabe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data paths\n",
    "data_dirpath = 'dataset'\n",
    "train_name = 'train.csv'\n",
    "test_name = 'test.csv'\n",
    "\n",
    "train_path = os.path.join(data_dirpath, train_name)\n",
    "test_path = os.path.join(data_dirpath, test_name)\n",
    "train_df = pd.read_csv(train_path, header=[0])\n",
    "test_df = pd.read_csv(test_path, header=[0])\n",
    "print(f'[Default] Number of train data: {train_df.shape[0]}, Number of test data: {test_df.shape[0]}')\n",
    "\n",
    "#mreplacing male, female by zeor and one and separating the data from its label\n",
    "lead_map = {'Female': 0 , 'Male': 1}\n",
    "train_df['Lead'] = train_df['Lead'].map(lead_map).astype(int)\n",
    "\n",
    "X_train = train_df.loc[:, train_df.columns != 'Lead']\n",
    "y_train = train_df['Lead']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cea778-c6eb-4565-a7af-119a851abfdb",
   "metadata": {},
   "source": [
    "### Computing some more features for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a04e6f-bea3-448a-b33d-86fef63b6d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ration of males to females\n",
    "X_train['female to male ratio'] = (X_train['Number words male']+1) /( X_train['Number words female']+1 )  \n",
    "\n",
    "#the lead, colead Age difference\n",
    "X_train['Age difference'] = X_train['Age Lead'] - X_train['Age Co-Lead']\n",
    "\n",
    "#scaling the gross by the year in order to better account for inflation\n",
    "X_train[\"YearXGross\"] = (np.max(X_train[\"Year\"].values))- X_train[\"Year\"] * X_train[\"Gross\"]\n",
    "\n",
    "# words per actor for each gender:\n",
    "X_train[\"words/actors female\"] = X_train[\"Number words female\"] / X_train[\"Number of female actors\"]\n",
    "X_train[\"words/actors male\"] = X_train[\"Number words male\"] / X_train[\"Number of male actors\"]\n",
    "\n",
    "# ratio of males to females, +1 because of division by zero\n",
    "X_train[\"Male/Female Actors Ratio\"] = (X_train['Number of male actors']+1) /(X_train['Number of female actors']+1)\n",
    "\n",
    "# Total number of actors\n",
    "X_train[\"Total Actors\"] = X_train['Number of male actors']+X_train['Number of female actors']\n",
    "\n",
    "X_train.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e00993-138a-4437-bb53-2a48fd4fd56e",
   "metadata": {},
   "source": [
    "### scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5bbdd2-e9f7-4f78-9948-1a953538fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some features need to be grouped like all counts for words, otherwise their proportionality would be lost\n",
    "combined_normalization = [\n",
    "       ['Number words female','Number words male','Total words', 'Number of words lead','Difference in words lead and co-lead','words/actors female', 'words/actors male'],\n",
    "       ['Number of male actors','Number of female actors', 'Total Actors'],\n",
    "       ['Year'],\n",
    "       ['Gross'],\n",
    "       ['Mean Age Male', 'Mean Age Female', 'Age Lead', 'Age Co-Lead','Age difference'],\n",
    "       ['female to male ratio'], \n",
    "       ['YearXGross'], \n",
    "       ['Male/Female Actors Ratio'],\n",
    " ]\n",
    "\n",
    "\n",
    "for combined in combined_normalization:\n",
    "    if len(combined) == 1:\n",
    "        X_train[combined] = normalize(X_train[combined],axis = 0)\n",
    "    else:\n",
    "        X_train[combined] = normalize(X_train[combined],axis = 1)\n",
    "                           \n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42813ad3-e7e3-4879-a80e-1a4c79d441ee",
   "metadata": {},
   "source": [
    "### setting up KNN and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2a783-4311-45fd-96f7-c3455d51683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = KDTree.valid_metrics\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier()\n",
    "knn.algorithm = 'kd_tree'\n",
    "knn.metric = metrics[0]\n",
    "knn.weights ='uniform' #'distance'\n",
    "knn.n_neighbors = 5\n",
    "knn.n_jobs = multiprocessing.cpu_count()-1 # to enable multithreading\n",
    "\n",
    "\n",
    "# \n",
    "# feature is a list of features to drop from the specified training set. \n",
    "def evaluate_feature( training_set,feature_list = [], score_to_return = 'accuracy', n_neighbors = [3,5,7,8,9,10,11] , metrics = ['euclidean',  'chebyshev', 'infinity'], neighbors_model = knn ):\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    \n",
    "    # setting up the data for crossvalidation\n",
    "    X_train_restricted = training_set.drop(columns=feature) # here features are dropped. [] if no features are dropped\n",
    "    crossValSegs = StratifiedKFold(n_splits=5,random_state = 42,shuffle = True).split(X_train_restricted,y_train) # setting up crossvalidation\n",
    "    crossValSegs = list(crossValSegs)\n",
    "    \n",
    "    #initializing scoring measures\n",
    "    scores_max = {\n",
    "        'accuracy' : 0,\n",
    "        'roc_auc' : 0,\n",
    "        'precision': 0,\n",
    "        'recall' : 0,\n",
    "        'f1' : 0\n",
    "    }\n",
    "    max_confusion = []\n",
    "    max_model_String = ''\n",
    "    scores_all = {}\n",
    "    \n",
    "\n",
    "    for metric in metrics: #[KDTree.valid_metrics]: include all metrics if you wish\n",
    "        neighbors_model.metric = metric\n",
    "        for n in   n_neighbors: \n",
    "            scores_n ={ \n",
    "            'accuracy' : [],\n",
    "            'roc_auc' :  [],\n",
    "            'f1'       : [],\n",
    "            'precision': [],\n",
    "            'recall' : [],\n",
    "            'confusion' : []\n",
    "            }\n",
    "            \n",
    "            neighbors_model.n_neighbors = n\n",
    "            for train, test in crossValSegs:\n",
    "                neighbors_model.fit(X_train_restricted.values[train],y_train.values[train])\n",
    "                probabilities = neighbors_model.predict_proba(X_train_restricted.values[test] )\n",
    "                y_pred = neighbors_model.predict(X_train_restricted.values[test])\n",
    "               \n",
    "                scores_n['accuracy'].append(accuracy_score(y_train.values[test], y_pred))\n",
    "                scores_n['f1'].append( f1_score(y_train.values[test], y_pred))\n",
    "                scores_n['roc_auc'].append(roc_auc_score(y_train.values[test], y_pred)) \n",
    "                scores_n['confusion'].append(confusion_matrix(y_train.values[test], y_pred))\n",
    "                scores_n['precision'].append(precision_score(y_train.values[test], y_pred))\n",
    "                scores_n['recall'].append(recall_score(y_train.values[test], y_pred))\n",
    "                \n",
    "            mean_score = np.mean(scores_n[score_to_return])\n",
    "            mean_confusion = np.mean(scores_n['confusion'],0)\n",
    "            current =  f'n = {n} metric = {metric}     Features dropped = {feature}   Mean {score_to_return} : {mean_score}, confusion = {mean_confusion}' \n",
    "            if mean_score > scores_max[score_to_return]: \n",
    "                scores_max[score_to_return] = mean_score\n",
    "                max_model_String = current\n",
    "                max_confusion  = mean_confusion\n",
    "                scores_all = scores_n.copy()\n",
    "               \n",
    "    scores_dict = { \n",
    "            'accuracy' : np.mean(scores_all['accuracy']),\n",
    "            'roc_auc' :  np.mean(scores_all['roc_auc']),\n",
    "            'f1'       : np.mean(scores_all['f1']),\n",
    "            'precision': np.mean(scores_all['precision']),\n",
    "            'recall' : np.mean(scores_all['recall']),\n",
    "            'confusion' : np.mean(scores_all['confusion'],0)\n",
    "            }\n",
    "\n",
    "    print(f'\\n\\nThe Best Model Is {max_model_String}')\n",
    "    scores = np.mean(scores_n[score_to_return])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return scores_max[score_to_return], max_model_String, scores_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5821905f-d3cd-4afa-95f1-a11023c51363",
   "metadata": {},
   "source": [
    "#### evaluate dropping different features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017bc8b0-4d6a-4527-a9b3-60526ee5106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop each feature iteratively and evaluate the change in accuracy\n",
    "all_features =  X_train.columns.tolist()\n",
    "feature_score = []\n",
    "feature_strings = []\n",
    "score_dicts = []\n",
    "score = 'roc_auc' # 'accuracy' 'roc_auc' ,'f1','precision', 'recall' \n",
    "for feature in all_features:\n",
    "    print(f'current Feature = {feature}')\n",
    "    max_accuracy, max_string, score_dict = evaluate_feature( X_train,[feature], score_to_return = score, metrics = ['euclidean',  'chebyshev', 'infinity'], neighbors_model = knn ) #['euclidean',  'chebyshev', 'infinity']\n",
    "     \n",
    "    feature_score.append(max_accuracy)\n",
    "    feature_strings.append(max_string)\n",
    "    score_dicts.append(score_dict.copy())\n",
    "    \n",
    "    \n",
    "    \n",
    "normal_score, _ ,_ = evaluate_feature( X_train,[], score_to_return = score, metrics = ['euclidean',  'chebyshev', 'infinity'], neighbors_model = knn )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085b9eb4-2b73-455a-9a98-c74b0d09ed12",
   "metadata": {},
   "source": [
    "#### plotting the way that dropping different features affects the score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680021fc-4995-43fa-a727-8d1126e279d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "all_features =  X_train.columns.tolist()\n",
    "\n",
    "\n",
    "bestModel = feature_strings[np.argmax(feature_score)]\n",
    "bestScores = score_dicts[np.argmax(feature_score)]\n",
    "\n",
    "print(f'Best performance:\\n {bestModel}')\n",
    "print(f'best scores for best Model {bestScores}')\n",
    "plt.barh(all_features, ((np.array(feature_score)-normal_score)*1))\n",
    "plt.xlabel(f'improvement of {score} score by dropping features')\n",
    "plt.ylabel('dropped feature')\n",
    "\n",
    "plt.savefig('Figures/featureImportance_' + score + '.png', dpi='figure', format='png', \n",
    "        bbox_inches='tight', pad_inches=0.1,\n",
    "        facecolor='auto', edgecolor='auto',\n",
    "        backend=None\n",
    "       )\n",
    "plt.show()\n",
    "\n",
    "feature_score = np.array(feature_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# remove all features that decrease the accuracy if they are included\n",
    "feature_usefull = (feature_score-normal_score) > 0 \n",
    "\n",
    "features =  [feat for usefull,feat in zip(feature_usefull,all_features) if   usefull]\n",
    "print(features)\n",
    "max_accuracy, max_string, _  = evaluate_feature( X_train,features, score_to_return = score, metrics = ['euclidean',  'chebyshev', 'infinity'], neighbors_model = knn )\n",
    "    \n",
    "print('\\n\\n',max_accuracy, max_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
